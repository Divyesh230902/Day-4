{"cells":[{"cell_type":"markdown","source":["# Basics of Transformers and Datasets libraries"],"metadata":{"id":"BTiSXyNEv9Qg"}},{"cell_type":"markdown","metadata":{"id":"p9vEQyg9ZB_0"},"source":["Installing the Transformers, Datasets, and Evaluate libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDvwkVcNZB_1"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BRmjwdw2ZB_2"},"outputs":[],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")\n","classifier(\"Welcome to the STTP - DEEP LEARNING ON PARAM SHAVAK\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LjKaZHPZB_4"},"outputs":[],"source":["from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\",max_length=60,num_return_sequences=5)\n","generator(\"In this STTP, we will learn how to\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Th_qqL9wZB_5"},"outputs":[],"source":["from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n","generator(\n","    \"In this STTP, we will teach you how to\",\n","    max_length=20,\n","    num_return_sequences=2,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbPWv0kBZB_6"},"outputs":[],"source":["from transformers import pipeline\n","\n","unmasker = pipeline(\"fill-mask\")\n","unmasker(\"This STTP will teach you all about <mask> models.\", top_k=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzR0qcUtZB_7"},"outputs":[],"source":["from transformers import pipeline\n","\n","ner = pipeline(\"ner\", grouped_entities=True)\n","ner(\"My name is Hariom Pandya and I work at Dharmsinh Desai University Nadiad.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mS309tnZB_7"},"outputs":[],"source":["from transformers import pipeline\n","\n","question_answerer = pipeline(\"question-answering\")\n","question_answerer(\n","    question=\"Where do I work?\",\n","    context=\"My name is Hariom and I work at Dharmsinh Desai University Nadiad.\",\n","    \n",")"]},{"cell_type":"code","source":["text=\"\"\"The ‘Param Shavak’ super computer is provided by GUJCOST to perform high-end computations \n","for scientific, engineering and academic programs and to address and catalyze the research using\n","modeling, simulation and data analysis. This STTP aims at spreading awareness about the\n","supercomputer facility ‘Param Shavak' and to use it for conducting deep learning based experiments\n","in the area of Natural Language Processing.\n","\n","Deep Learning is the current buzzword in the field of AI. Advancements in Hardware\n","technologies and availability of humongous data made the realization of deep learning possible.\n","Various educational and research institutes as well as IT industries have started incorporating\n","the technology. This STTP is a five days training course for research scholars, academicians and industry professionals. \n","The participants will be given hands-on training sessions in the state-of-the-art laboratories of the department. \n","Eminent experts are invited to conduct their expert sessions and train the participants.\"\"\"\n","\n","question=\"Who are the target audience of STTP?\"\n","\n","question_answerer(\n","    question=question,\n","    context=text   \n",")"],"metadata":{"id":"hVo0LxyXw4mI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question-Answering System"],"metadata":{"id":"bXU8EwYNwFl4"}},{"cell_type":"markdown","source":["##Loading Dataset"],"metadata":{"id":"OXmQt64pMfBx"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"squad\")"],"metadata":{"id":"hq8XTiub4uT2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_datasets"],"metadata":{"id":"Secq8tIN4xJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["context=raw_datasets[\"train\"][0][\"context\"]\n","question=raw_datasets[\"train\"][0][\"question\"]\n","print(\"Context: \", context)\n","print(\"Question: \", question)\n","print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])"],"metadata":{"id":"yB2ixOVQ40vC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Tokenizer"],"metadata":{"id":"J2ecduCEMmAk"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","model_checkpoint = \"bert-base-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"],"metadata":{"id":"2kgLISYb5FB4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[CLS] question [SEP] context [SEP] format"],"metadata":{"id":"tc01rw576M0i"}},{"cell_type":"code","source":["inputs = tokenizer(question, context)\n","tokenizer.decode(inputs[\"input_ids\"])"],"metadata":{"id":"IE5zn8Q55JXx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Processing Training Data\n","  1. Limit the length to 100\n","  2. sliding window of 50 tokens\n","  3. return_overflowing_tokens=True to let the tokenizer know we want the overflowing tokens"],"metadata":{"id":"1jUXyqmq6k0Q"}},{"cell_type":"code","source":["inputs = tokenizer(\n","    question,\n","    context,\n","    max_length=100,\n","    truncation=\"only_second\",\n","    stride=50,\n","    return_overflowing_tokens=True,\n",")\n","\n","\n","for ids in inputs[\"input_ids\"]:\n","    print(tokenizer.decode(ids))\n","\n","    "],"metadata":{"id":"uiTpthKZ5cgz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The answer to the question (“Bernadette Soubirous”) only appears in the third and last inputs\n","\n","    1. Approach will create some training examples where the answer is not included in the context. \n","      For those examples, the labels: start_position = end_position = 0 \n","      (so we predict the [CLS] token). \n","    2. We will also have labels in the case where the answer has been truncated so that we only have the start (or end) of it. \n","    3. For the examples where the answer is fully in the context, \n","      The labels will be the index of answer start and the index of answer end."],"metadata":{"id":"O7x91ayW_apg"}},{"cell_type":"markdown","source":["\n","*   The dataset provides us start character of the answer in the context \n","*   By adding the length of the answer, we can find the end character in the context.\n","* Next, to map those to token indices, we will need to use the offset mappings \n","\n","\n","As shown in next example:\n","tokenizer is already keeping track of the offsets, this is how it can give us the offset mapping\n","\n","\n","\n"],"metadata":{"id":"AWS7P_Qh_6aA"}},{"cell_type":"markdown","source":["###Offset mapping"],"metadata":{"id":"EuFtDxLeB69i"}},{"cell_type":"code","source":["tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"],"metadata":{"id":"jZiaDelLAu_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer(\n","    question,\n","    context,\n","    max_length=100,\n","    truncation=\"only_second\",\n","    stride=50,\n","    return_overflowing_tokens=True,\n","    return_offsets_mapping=True,\n",")\n","inputs.keys()"],"metadata":{"id":"46pbjFj2B-9V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###overflow to sample mapping\n","\n","\n","\n","*   Useful when we tokenize several texts at the same time\n","*   Since one sample can give several features, it maps each feature to the example it originated from\n","\n"," As here we only tokenized one example, we get a list of 0s:"],"metadata":{"id":"Dm1Fcpd7CHvZ"}},{"cell_type":"code","source":["inputs[\"overflow_to_sample_mapping\"]"],"metadata":{"id":"dJdT9d1uCgxP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Helpful while we have multiple examples.\n","\n","In next demo the first three examples (at indices 2, 3, and 4 in the training set) each gave four features and the last example (at index 5 in the training set) gave 7 features."],"metadata":{"id":"6t9wUySRCtf4"}},{"cell_type":"code","source":["inputs = tokenizer(\n","    raw_datasets[\"train\"][2:6][\"question\"],\n","    raw_datasets[\"train\"][2:6][\"context\"],\n","    max_length=100,\n","    truncation=\"only_second\",\n","    stride=50,\n","    return_overflowing_tokens=True,\n","    return_offsets_mapping=True,\n",")\n","\n","print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n","print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"],"metadata":{"id":"fcVvY8JYCxBv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This information will be useful to map each feature to its corresponding label.\n","Labels are :\n","\n","    1. (0, 0) if the answer is not in the corresponding span of the context OR\n","    2. (start_position, end_position) if the answer is in the corresponding span of the context"],"metadata":{"id":"N-kFHBfjDHLg"}},{"cell_type":"markdown","source":["\n","1.   Now, we find the indices that start and end the context in the input IDs using sequence_ids()\n","\n","2.   we look at the corresponding offsets, which are tuples of two integers representing the span of characters inside the original context.\n","\n","3. We loop to find the first and last token of the answer:\n","\n"],"metadata":{"id":"ALLYEzx3D9MQ"}},{"cell_type":"code","source":["answers = raw_datasets[\"train\"][2:6][\"answers\"]\n","start_positions = []\n","end_positions = []\n","\n","for i, offset in enumerate(inputs[\"offset_mapping\"]):\n","    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n","    answer = answers[sample_idx]\n","    start_char = answer[\"answer_start\"][0]\n","    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","    sequence_ids = inputs.sequence_ids(i)\n","\n","    # Find the start and end of the context\n","    idx = 0\n","    while sequence_ids[idx] != 1:\n","        idx += 1\n","    context_start = idx\n","    while sequence_ids[idx] == 1:\n","        idx += 1\n","    context_end = idx - 1\n","\n","    # If the answer is not fully inside the context, label is (0, 0)\n","    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n","        start_positions.append(0)\n","        end_positions.append(0)\n","    else:\n","        # Otherwise it's the start and end token positions\n","        idx = context_start\n","        while idx <= context_end and offset[idx][0] <= start_char:\n","            idx += 1\n","        start_positions.append(idx - 1)\n","\n","        idx = context_end\n","        while idx >= context_start and offset[idx][1] >= end_char:\n","            idx -= 1\n","        end_positions.append(idx + 1)\n","\n","start_positions, end_positions"],"metadata":{"id":"Of0nSFgAD6qS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's Verify our results"],"metadata":{"id":"ogVbMaSpFyr1"}},{"cell_type":"code","source":["#For the first feature we find (83, 85) as labels\n","\n","idx = 0\n","sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n","answer = answers[sample_idx][\"text\"][0]\n","\n","start = start_positions[idx]\n","end = end_positions[idx]\n","labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n","\n","print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")\n","\n","#For index 4 labels is (0, 0), which means the answer is not in the context chunk of that feature:\n","idx = 4\n","sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n","answer = answers[sample_idx][\"text\"][0]\n","\n","decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n","print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")"],"metadata":{"id":"UcFaJoFeFx5T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Combining all steps in single function"],"metadata":{"id":"s1EjIXP1GQgw"}},{"cell_type":"code","source":["max_length = 384\n","stride = 128\n","\n","\n","def preprocess_training_examples(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=max_length,\n","        truncation=\"only_second\",\n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offset in enumerate(offset_mapping):\n","        sample_idx = sample_map[i]\n","        answer = answers[sample_idx]\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Find the start and end of the context\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","        # If the answer is not fully inside the context, label is (0, 0)\n","        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Otherwise it's the start and end token positions\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","    return inputs"],"metadata":{"id":"WYZ8qL_OGTtn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Apply this function to the whole training set"],"metadata":{"id":"bZS4QsO5GeXW"}},{"cell_type":"code","source":["#range specify training only on 1000 examples\n","train_dataset = raw_datasets[\"train\"].select(range(1000)).map(\n","    preprocess_training_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"train\"].column_names,\n",")\n","len(raw_datasets[\"train\"]), len(train_dataset)"],"metadata":{"id":"KXOxL-XaGb8W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Finetuning"],"metadata":{"id":"1hg0oxWKQriN"}},{"cell_type":"code","source":["from transformers import BertForQuestionAnswering, TrainingArguments, Trainer\n","\n","model = BertForQuestionAnswering.from_pretrained(model_checkpoint)\n","\n","args = TrainingArguments(\n","    output_dir='.',\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=1,\n",")\n","\n","from transformers import default_data_collator\n","data_collator = default_data_collator\n","\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")\n","\n","trainer.train()"],"metadata":{"id":"i10wmkYTTmac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(\".\")\n"],"metadata":{"id":"e5TFjdAjallR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question_answerer = pipeline(\"question-answering\",model=\".\")\n","question_answerer(\n","    question=\"Which transformer model you are using?\",\n","    context=\"I am using BERT transformer model.\",\n",")"],"metadata":{"id":"VKF3g8DKUBra"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/master/course/chapter1/section3.ipynb","timestamp":1662378481556}]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}